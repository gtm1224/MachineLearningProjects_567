{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "\n",
    "This is your playground to test the Q-learning agent. You can modify anything you want in this file. It will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from q_learning import QLearningAgent\n",
    "from utils import generate_random_mdp\n",
    "\n",
    "# What is autoreload? See: https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may set seed here for reproducibility\n",
    "SEED = 0\n",
    "N_ROUNDS = 10000   \n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "env = generate_random_mdp(n_states=3, n_actions=2, sparse_reward=False)\n",
    "\n",
    "# Feel free to play around with different hyperparameters. \n",
    "# (E.g., try buffer_capacity=1, which is equivalent to Q-learning without experience replay)\n",
    "agent = QLearningAgent(\n",
    "    state_space=env.state_space,\n",
    "    action_space=env.action_space,\n",
    "    lr=0.1,\n",
    "    discount=0.99,\n",
    "    explore_rate=0.5,\n",
    "    buffer_capacity=1000,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "reports = []\n",
    "state, info = env.reset()\n",
    "for t in range(N_ROUNDS):\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    agent.observe(state, action, reward, next_state)\n",
    "    report = agent.learn()\n",
    "    report.update({'round': t, 'reward': reward})\n",
    "    reports.append(report)\n",
    "    if done:\n",
    "        state, info = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "reports[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([report['reward'] for report in reports])\n",
    "rounds = np.arange(1, N_ROUNDS + 1)\n",
    "running_avg = rewards.cumsum() / rounds\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(rounds, running_avg, label='Running Average Reward')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title(f'Learning curve on random MDP with {env.state_space.n} states and {env.action_space.n} actions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- We may expect the RL agent to obtain higher running-average reward as it interacts with the environment.\n",
    "- Indeed, for a small randomly generated MDP (e.g., 3 states, 2 actions), with appropriate hyper-parameters, the running-average reward should converge after 1,000 rounds. See below for an example:\n",
    "\n",
    "![example_learning_curve.png](./example_learning_curve.png)\n",
    "\n",
    "- However, it is possible to see occasional drops in performance. Vanilla Q-Learning is not guaranteed to converge in general. \n",
    "- For non-trivial environments, an online RL agent may also suffer from [*catatraphic forgetting*](https://en.wikipedia.org/wiki/Catastrophic_interference)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
